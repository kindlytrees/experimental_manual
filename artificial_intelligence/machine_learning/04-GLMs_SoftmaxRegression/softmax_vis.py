# !python 
# generated by gemini

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.linear_model import LogisticRegression
from matplotlib.colors import ListedColormap

import matplotlib.pyplot as plt
import matplotlib

# 设置全局字体为黑体（SimHei）
matplotlib.rcParams['font.family'] = 'SimHei'
matplotlib.rcParams['axes.unicode_minus'] = False  # 正确显示负号

# --- 1. 生成模拟的三分类数据 ---
n_samples = 300
n_features = 2 # 2D数据方便可视化
#n_classes = 5
n_classes = 3
X, y = make_blobs(n_samples=n_samples, n_features=n_features, centers=n_classes,
                  cluster_std=1.0, random_state=42)

# --- 2. 训练Softmax回归模型 ---
# LogisticRegression with multi_class='multinomial' and solver='lbfgs'
# implements Softmax Regression.
# C is the inverse of regularization strength; larger C means less regularization.
# We use a large C to allow the model to fit the data more closely.
model = LogisticRegression(multi_class='multinomial', solver='lbfgs', C=100, random_state=42)
model.fit(X, y)

print(f"模型训练完成。")
print(f"权重 (coef_):\n{model.coef_}")
print(f"偏置 (intercept_):\n{model.intercept_}")

# --- 3. 可视化决策边界和分类区域 ---

# 定义绘图区域的网格
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),
                     np.arange(y_min, y_max, 0.02))

# 预测网格中每个点的类别，用于绘制分类区域
Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

# 自定义颜色映射
# cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF', '#AA00FF', '#AAA00F']) # 浅色用于区域
# cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF', '#0AAA0F', '#A00AFF'])   # 深色用于数据点
cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF']) # 浅色用于区域
cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])   # 深色用于数据点

plt.figure(figsize=(10, 8))

# 绘制分类区域
plt.contourf(xx, yy, Z, cmap=cmap_light, alpha=0.8)

# 绘制数据点
plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold, edgecolor='k', s=60, label="数据点")

# --- 4. 绘制线性决策边界 ---
# Softmax回归在二维空间中有 n_classes * (n_classes - 1) / 2 条决策边界
# 例如，3个类别有 3 * 2 / 2 = 3 条边界 (0-1, 0-2, 1-2)

colors = ['purple', 'orange', 'cyan', 'red', 'green'] # 区分不同决策边界的颜色

for i in range(n_classes):
    for j in range(i + 1, n_classes):
        # 决策边界方程: (w_i^T x + b_i) = (w_j^T x + b_j)
        # 整理为: (w_i[0] - w_j[0])x1 + (w_i[1] - w_j[1])x2 + (b_i - b_j) = 0
        
        # 获取第i个类别和第j个类别的权重和偏置
        w_i = model.coef_[i]
        b_i = model.intercept_[i]
        w_j = model.coef_[j]
        b_j = model.intercept_[j]

        # 计算差值
        w_diff = w_i - w_j
        b_diff = b_i - b_j

        # 边界方程: Ax + By + C = 0, 其中 A = w_diff[0], B = w_diff[1], C = b_diff
        
        # 绘制这条直线
        # 为了确保直线覆盖整个绘图区域，我们使用绘图区域的 x 范围
        line_x = np.array([x_min, x_max])
        
        # 处理垂直线（当 B 接近于0时）
        if np.isclose(w_diff[1], 0):
            # 垂直线 x = -C/A
            line_y = np.array([-b_diff / w_diff[0], -b_diff / w_diff[0]])
            plt.axvline(x=-b_diff / w_diff[0], color=colors[(i+j)%len(colors)], linestyle='--', 
                        label=f'边界 {i}-{j}: $z_{i} = z_{j}$')
        else:
            # y = (-A/B)x - (C/B)
            line_y = (-w_diff[0] * line_x - b_diff) / w_diff[1]
            plt.plot(line_x, line_y, color=colors[(i+j)%len(colors)], linestyle='--', 
                     label=f'边界 {i}-{j}: $z_{i} = z_{j}$')


plt.xlabel("特征 1")
plt.ylabel("特征 2")
plt.title("Softmax 回归分类可视化 (三分类)")
plt.xlim(xx.min(), xx.max())
plt.ylim(yy.min(), yy.max())
plt.legend()
plt.grid(True, linestyle=':', alpha=0.7)
plt.show()