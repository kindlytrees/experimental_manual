{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ui3LiQ4HdrnA"
      },
      "source": [
        "# Question answering (PyTorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSlT-aF3drnG"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zE2CH8t8drnH"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]\n",
        "!pip install accelerate\n",
        "# To run the training on TPU, you will need to uncomment the following line:\n",
        "# !pip install cloud-tpu-client==0.10 torch==1.9.0 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl\n",
        "!apt install git-lfs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsNwA-FjdrnJ"
      },
      "source": [
        "You will need to setup git, adapt your email and name in the following cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N39_ybazdrnJ"
      },
      "outputs": [],
      "source": [
        "!git config --global user.email \"kindlytree@163.com\"\n",
        "!git config --global user.name \"kindlytree\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ivf7RU6VdrnK"
      },
      "source": [
        "You will also need to be logged in to the Hugging Face Hub. Execute the following and enter your credentials."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0Xp_LH-drnK"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxAZ-mrHdrnM"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "raw_datasets = load_dataset(\"squad\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8gKjBT84drnM",
        "outputId": "f5329901-6b06-455c-e053-034d8e1311f2"
      },
      "outputs": [],
      "source": [
        "raw_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QBZld0BdrnO",
        "outputId": "f5356d78-f686-40f0-db49-770658ecf19b"
      },
      "outputs": [],
      "source": [
        "print(\"Context: \", raw_datasets[\"train\"][0][\"context\"])\n",
        "print(\"Question: \", raw_datasets[\"train\"][0][\"question\"])\n",
        "print(\"Answer: \", raw_datasets[\"train\"][0][\"answers\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNH1UcJedrnO",
        "outputId": "02aab1f3-7226-4a5a-97da-5299cdbde09f"
      },
      "outputs": [],
      "source": [
        "raw_datasets[\"train\"].filter(lambda x: len(x[\"answers\"][\"text\"]) != 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "validation Êï∞ÊçÆÈõÜ‰∏≠Ôºåanswers Â≠óÊÆµÈÄöÂ∏∏ÂåÖÂê´‰∏Ä‰∏™ text ÁöÑÂàóË°®ÔºåÂéüÂõ†‰∏ªË¶ÅÊúâ:Âú®ËØÑ‰º∞Ê®°ÂûãÊó∂ÔºåÂ¶ÇÊûúÊ®°ÂûãÁöÑËæìÂá∫Á≠îÊ°à‰∏é answers['text'] ÂàóË°®‰∏≠ÁöÑ‰ªª‰Ωï‰∏Ä‰∏™ÂåπÈÖçÔºåÈÄöÂ∏∏Â∞±Ë¢´ËÆ§‰∏∫ÊòØÊ≠£Á°ÆÁöÑ„ÄÇÂõ†Ê≠§ÔºåÊèê‰æõÂ§ö‰∏™ÂèÇËÄÉÁ≠îÊ°àÂèØ‰ª•ËÆ©Ê®°ÂûãÂú®ËØÑ‰º∞Êó∂Êõ¥ÂÖ∑ÁÅµÊ¥ªÊÄßÔºåÂáèÂ∞ëÂõ†Ê†ºÂºèÊàñË°®Ëø∞Â∑ÆÂºÇÂ∏¶Êù•ÁöÑ‰∏çÂøÖË¶ÅÁöÑÊÉ©ÁΩö„ÄÇÊúâÊó∂ÔºåÊï∞ÊçÆÈõÜ‰ºöÂú® answers['text'] ÂàóË°®‰∏≠ÂåÖÂê´Áõ∏ÂêåÊàñÈùûÂ∏∏Áõ∏‰ººÁöÑÁ≠îÊ°à„ÄÇËøôÊ†∑ÁöÑÂÜó‰ΩôÂèØËÉΩÊòØ‰∏∫‰∫ÜÂ¢ûÂº∫Ê®°ÂûãÂú®Èù¢ÂØπ‰∏çÂÆåÁæéÊàñÈáçÂ§çÁ≠îÊ°àÊó∂ÁöÑÈ≤ÅÊ£íÊÄß„ÄÇ\n",
        "Âèò‰ΩìÔºö‰∏çÂêåÁöÑ‰∫∫ÂèØËÉΩ‰ºö‰ª•‰∏çÂêåÁöÑÊñπÂºèÂõûÁ≠îÁõ∏ÂêåÁöÑÈóÆÈ¢ò„ÄÇ‰æãÂ¶ÇÔºåÂØπ‰∫éÈóÆÈ¢ò \"What is the capital of France?\"ÔºåÂèØËÉΩÁöÑÊ≠£Á°ÆÁ≠îÊ°àÊúâ \"Paris\", \"The capital of France is Paris\", ÊàñËÄÖÂè™ÊòØÁÆÄÂçïÁöÑ \"Paris.\"„ÄÇÂõ†Ê≠§Ôºåanswers['text'] ‰∏≠ÂèØËÉΩ‰ºöÂåÖÂê´Â§ö‰∏™Âèò‰ΩìÁ≠îÊ°àÔºå‰ª•Ë¶ÜÁõñÊâÄÊúâÂêàÁêÜÁöÑÂõûÁ≠î„ÄÇ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mpiu0GINdrnP",
        "outputId": "bd7ee5d9-9814-4c27-b29c-1a2fdcb71af8"
      },
      "outputs": [],
      "source": [
        "print(raw_datasets[\"validation\"][0][\"answers\"])\n",
        "print(raw_datasets[\"validation\"][2][\"answers\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BruMdux9drnP",
        "outputId": "4468531f-1498-4854-f0d4-964d36b4c4ee"
      },
      "outputs": [],
      "source": [
        "print(raw_datasets[\"validation\"][2][\"context\"])\n",
        "print(raw_datasets[\"validation\"][2][\"question\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Js6neWbmdrnP"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_checkpoint = \"bert-base-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zky1WDEHdrnQ",
        "outputId": "3b776571-5a41-448b-c671-08b76b4fee7e"
      },
      "outputs": [],
      "source": [
        "tokenizer.is_fast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxQ1ykqsdrnQ",
        "outputId": "5fde218f-ffb9-47e8-b992-5b6deecc56d6"
      },
      "outputs": [],
      "source": [
        "context = raw_datasets[\"train\"][0][\"context\"]\n",
        "question = raw_datasets[\"train\"][0][\"question\"]\n",
        "\n",
        "inputs = tokenizer(question, context)\n",
        "tokenizer.decode(inputs[\"input_ids\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ÈôêÂÆö‰∫Ümax_length=100Ôºå‰∏ãÈù¢ËøêË°åÁöÑÁªìÊûú‰∏≠Ôºå‰∏ã‰∏Ä‰∏™Âè•Â≠êÁöÑÂºÄÂ§¥50‰∏™tokenÂíå‰∏ä‰∏ÄÂè•ÁöÑÊúÄÂêé50‰∏™ÊòØoveralapÁöÑ„ÄÇ\n",
        "truncation=\"only_second\",Ë°®ÊòéÂè™Âú®Á¨¨‰∫åÈÉ®ÂàÜÁöÑÂèÇÊï∞‰∏äÂéªËøõË°åÊà™Êñ≠Ôºå‰πüÂç≥ÊòØÈóÆÈ¢òÊú¨Ë∫´‰∏çÊà™Êñ≠"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jkxq7bjjdrnQ",
        "outputId": "dbe4caec-bce5-4b73-c3ba-a8fbd5a7bf12"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer(\n",
        "    question,\n",
        "    context,\n",
        "    max_length=100,\n",
        "    truncation=\"only_second\",\n",
        "    stride=50,\n",
        "    return_overflowing_tokens=True,\n",
        ")\n",
        "\n",
        "for ids in inputs[\"input_ids\"]:\n",
        "    print(tokenizer.decode(ids))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ÈïøÊñáÊú¨ÈóÆÁ≠îÔºö\n",
        "\n",
        "ÂΩì‰Ω†Â∞ÜÈïøÊñáÊú¨ÂàÜÂâ≤ÊàêÂ§ö‰∏™ÁâáÊÆµ‰ª•‰æøÊ®°ÂûãÂ§ÑÁêÜÊó∂ÔºåÂèØ‰ª•‰ΩøÁî® overflow_to_sample_mapping Á°Æ‰øùÊâÄÊúâÁâáÊÆµÁöÑÁ≠îÊ°àÈÉΩËÉΩË¢´Ê≠£Á°ÆÊò†Â∞ÑÂõûÂØπÂ∫îÁöÑÂéüÂßãÈóÆÈ¢òÊñáÊú¨„ÄÇ\n",
        "ÊâπÈáèÂ§ÑÁêÜÈïøÊñáÊú¨Ôºö\n",
        "\n",
        "Âú®Â§ÑÁêÜ‰∏ÄÊâπÊñáÊú¨Êï∞ÊçÆÊó∂Ôºåoverflow_to_sample_mapping ÂèØ‰ª•Â∏ÆÂä©‰Ω†ËøΩË∏™Âì™‰∏™ÁâáÊÆµÂØπÂ∫îÂì™‰∏™ÂéüÂßãÊñáÊú¨Ê†∑Êú¨ÔºåËøôÂØπ‰∫éËÆ≠ÁªÉÂíåÊé®ÁêÜ‰ªªÂä°ÈùûÂ∏∏ÈáçË¶Å„ÄÇ\n",
        "ÊñáÊú¨Â§ÑÁêÜÂíåÂàÜÊûêÔºö\n",
        "\n",
        "‰ΩøÁî® overflow_to_sample_mapping ÂèØ‰ª•Â∏ÆÂä©Âú®ÈïøÊñáÊú¨ÁöÑÂ§ö‰∏™ÁâáÊÆµ‰∏≠‰øùÊåÅÂØπÂéüÂßãÊñáÊú¨ÁöÑÁêÜËß£ÂíåËøûÊé•ÔºåÁâπÂà´ÊòØÂú®Ê®°ÂûãËæìÂá∫ÁªìÊûúÈúÄË¶Å‰∏éÂéüÂßãËæìÂÖ•ÊñáÊú¨ÂÖ≥ËÅîÊó∂„ÄÇ\n",
        "\n",
        "‰∏ãÈù¢ÁöÑÁªìÊûú‰∏≠ÁîüÊàêÁöÑ4‰∏™Áõ∏ÂØπÁöÑÊ†∑Êú¨ÁöÑÁ¥¢ÂºïÈÉΩÊòØ0ÔºåÂíå‰∏äÈù¢ÁöÑÁªìÊûúÊòØ‰∏ÄËá¥ÁöÑ„ÄÇ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sI7gmF9hdrnQ",
        "outputId": "b795144c-ba44-4f14-8e44-ea756c4e2a54"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer(\n",
        "    question,\n",
        "    context,\n",
        "    max_length=100,\n",
        "    truncation=\"only_second\",\n",
        "    stride=50,\n",
        "    return_overflowing_tokens=True,\n",
        "    return_offsets_mapping=True,\n",
        ")\n",
        "inputs.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CT1kzpPEdrnR",
        "outputId": "a423d6a8-6fff-4c4b-e822-59fd497e0d55"
      },
      "outputs": [],
      "source": [
        "inputs[\"overflow_to_sample_mapping\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "tokenizer(text_a, text_b, ...) ËøôÁßçË∞ÉÁî®ÊñπÂºèÊòØ‰∏ìÈó®‰∏∫ÈúÄË¶ÅËæìÂÖ•‰∏ÄÂØπÊñáÊú¨ÁöÑ‰ªªÂä°ËÆæËÆ°ÁöÑÔºåÂ¶ÇÈóÆÁ≠îÔºàQuestion-ContextÔºâÊàñËá™ÁÑ∂ËØ≠Ë®ÄÊé®Êñ≠ÔºàPremise-HypothesisÔºâ„ÄÇ\n",
        "Tokenizer‰ºöËá™Âä®Â∞ÜÂÆÉ‰ª¨Ê†ºÂºèÂåñÊàêBERTÁ≠âÊ®°ÂûãÈúÄË¶ÅÁöÑÊ†ºÂºèÔºåÈÄöÂ∏∏ÊòØÔºö\n",
        "[CLS] question_tokens [SEP] context_tokens [SEP]  \n",
        "\n",
        "‰∏∫‰ªÄ‰πàÂú®QA‰∏≠Â∏∏Áî®\"only_second\"?: Âõ†‰∏∫ÈóÆÈ¢òÈÄöÂ∏∏ÂæàÁü≠‰∏îÂåÖÂê´ÂÖ≥ÈîÆ‰ø°ÊÅØÔºåÊàë‰ª¨‰∏çÂ∏åÊúõÊà™Êñ≠ÂÆÉ„ÄÇËÄå‰∏ä‰∏ãÊñáÈÄöÂ∏∏ÂæàÈïøÔºåÊà™Êñ≠ÂÆÉÊòØÂêàÁêÜÁöÑ„ÄÇÊàë‰ª¨ÂÆÅÊÑø‰∏¢Â§±ÈÉ®ÂàÜ‰∏ä‰∏ãÊñáÔºå‰πü‰∏çÊÑø‰∏¢Â§±ÈóÆÈ¢ò„ÄÇÂΩì‰∏Ä‰∏™‰∏ä‰∏ãÊñáÂõ†‰∏∫Â§™ÈïøËÄåË¢´ÂàáÂàÜÊàêÂ§ö‰∏™ÂùóÔºàchunkÔºâÊó∂ÔºåÂêé‰∏Ä‰∏™Âùó‰ºöÂåÖÂê´Ââç‰∏Ä‰∏™ÂùóÊú´Â∞æÁöÑ50‰∏™token„ÄÇ\n",
        "\n",
        "return_offsets_mapping=True\n",
        "‰ΩúÁî®: ËøôÊòØ‰∏Ä‰∏™ÈùûÂ∏∏ÊúâÁî®ÁöÑÂÖÉÊï∞ÊçÆÂäüËÉΩÔºåÂÆÉ‰∏∫ÊØè‰∏™tokenÊèê‰æõ‰∫ÜÂÖ∂Âú®ÂéüÂßãÊñáÊú¨‰∏≠ÁöÑËµ∑Ê≠¢Â≠óÁ¨¶‰ΩçÁΩÆ„ÄÇ\n",
        "ËæìÂá∫Ê†ºÂºè: inputs‰∏≠‰ºöÂåÖÂê´‰∏Ä‰∏™Âêç‰∏∫ offset_mapping ÁöÑÂ≠óÊÆµ„ÄÇÂÆÉÁöÑÂÄºÊòØ‰∏Ä‰∏™ÂàóË°®ÔºåÊØè‰∏™ÂÖÉÁ¥†ÂØπÂ∫î‰∏Ä‰∏™ÁâπÂæÅ„ÄÇÂØπ‰∫éÊØè‰∏™ÁâπÂæÅÔºåÂÖ∂ÂÄºÂèàÊòØ‰∏Ä‰∏™ÂàóË°®ÔºåÂÖ∂‰∏≠ÊØè‰∏™ÂÖÉÁ¥†ÊòØ‰∏Ä‰∏™ÂÖÉÁªÑ (start_char, end_char)„ÄÇ\n",
        "‰æãÂ¶Ç (0, 5) Ë°®Á§∫Ëøô‰∏™tokenÂØπÂ∫îÂéüÂßãÂ≠óÁ¨¶‰∏≤‰∏≠‰ªéÁ¥¢Âºï0Âà∞4ÁöÑÂ≠óÁ¨¶„ÄÇ\n",
        "ÂØπ‰∫éÁâπÊÆätokenÔºàÂ¶Ç[CLS], [SEP]ÔºâÔºå‰ª•Âèä‰∏çÂ±û‰∫éÊüê‰∏™ÂéüÂßãÊñáÊú¨ÔºàÊØîÂ¶ÇÈóÆÈ¢òÈÉ®ÂàÜtokenÁöÑoffset mappingÂú®‰∏ä‰∏ãÊñáÈÉ®ÂàÜ‰ºöÊòØ(0,0)ÔºâÁöÑtokenÔºåÂÖ∂ÂÅèÁßªÈáè‰∏∫ (0, 0)„ÄÇ  \n",
        "‰∏∫‰ªÄ‰πàÂú®QA‰∏≠Ëá≥ÂÖ≥ÈáçË¶Å?: Ê®°ÂûãÈ¢ÑÊµãÂá∫ÁöÑÁ≠îÊ°àÊòØstart_token_indexÂíåend_token_index„ÄÇÊàë‰ª¨ÈúÄË¶ÅÁî®offset_mappingÊù•Â∞ÜËøô‰∏§‰∏™tokenÁ∫ßÂà´ÁöÑÁ¥¢ÂºïÊò†Â∞ÑÂõûÂéüÂßã‰∏ä‰∏ãÊñáÂ≠óÁ¨¶‰∏≤‰∏≠ÁöÑ‰ΩçÁΩÆÔºå‰ªéËÄåÊäΩÂèñÂá∫‰∫∫Á±ªÂèØËØªÁöÑÁ≠îÊ°àÊñáÊú¨„ÄÇ  \n",
        "start_char = offset_mapping[start_token_index][0]  \n",
        "end_char = offset_mapping[end_token_index][1]  \n",
        "answer_text = context[start_char : end_char]  \n",
        "\n",
        "‰∏∫‰ªÄ‰πàÈúÄË¶ÅstrideÔºàÈáçÂè†Ôºâ?: ‰∏∫‰∫ÜÈò≤Ê≠¢Á≠îÊ°àÊÅ∞Â•ΩÂá∫Áé∞Âú®‰∏§‰∏™ÂùóÁöÑÂàáÂâ≤Â§ÑÔºåÂØºËá¥Á≠îÊ°àË¢´‚ÄúÂàáÊñ≠‚Äù„ÄÇÈÄöËøáÈáçÂè†ÔºåÂèØ‰ª•‰øùËØÅÁ≠îÊ°àÂÆåÊï¥Âú∞Âá∫Áé∞Âú®Ëá≥Â∞ë‰∏Ä‰∏™Âùó‰∏≠„ÄÇ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxEm5k6sdrnR",
        "outputId": "09e35997-08f1-4884-d457-aae4822f4007"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer(\n",
        "    raw_datasets[\"train\"][2:6][\"question\"],\n",
        "    raw_datasets[\"train\"][2:6][\"context\"],\n",
        "    max_length=100,\n",
        "    truncation=\"only_second\",\n",
        "    stride=50,\n",
        "    return_overflowing_tokens=True,\n",
        "    return_offsets_mapping=True,\n",
        ")\n",
        "\n",
        "print(f\"The 4 examples gave {len(inputs['input_ids'])} features.\")\n",
        "print(f\"Here is where each comes from: {inputs['overflow_to_sample_mapping']}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ËøôÊ¨°ÊúâÂõõ‰∏™ÈóÆÈ¢òÔºåÂØπÂ∫î‰∫éËÆ≠ÁªÉÈõÜ‰∏≠ÁöÑÁ¥¢Âºï‰∏∫2Ôºå3Ôºå4Ôºå5‰∏™ÈóÆÁ≠îÊï∞ÊçÆÔºåÊØè‰∏Ä‰∏™ÂàÜÂà´‰∫ßÁîü‰∫Ü4Ôºå4Ôºå4Ôºå7‰∏™ÁâáÊÆµÔºàÊØè‰∏™ÁâáÊÆµÂíå‰∏ä‰∏Ä‰∏™ÁâáÊÆµÊúâ50‰∏™tokenÁöÑÈáçÂè†ÔºâÔºå‰∏ç‰∏¢Â§±‰∏ä‰∏ãÊñá‰ø°ÊÅØÔºåÈóÆÈ¢ò‰∏çÊà™Êñ≠„ÄÇÊÄªÁöÑ‰∏ãÊù•Êúâ19‰∏™ËæìÂÖ•tokenÂ∫èÂàó„ÄÇ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(inputs['offset_mapping'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "offset_mapping ÊòØ Hugging Face ÁöÑ tokenizer Âú®ËøîÂõûÁöÑÁºñÁ†ÅÁªìÊûú‰∏≠Êèê‰æõÁöÑ‰∏Ä‰∏™ÂèÇÊï∞ÔºåÂÆÉÁî®‰∫éÊåáÁ§∫ÊØè‰∏™ token Âú®ÂéüÂßãÊñáÊú¨‰∏≠ÁöÑËµ∑ÂßãÂíåÁªìÊùüÂ≠óÁ¨¶Á¥¢Âºï„ÄÇ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "inputs['offset_mapping']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "offset_mappingÊòØÁõ∏ÂØπ‰∫éÊï¥‰∏™question+contextÁöÑÂ≠óÁ¨¶‰∏≤‰∏≠ÊØè‰∏™tokenÁöÑstartÂíåendÁöÑÁ¥¢Âºï,ÊòØ‰∏Ä‰∏™\"‰∫åÁª¥ÁöÑÂàóË°®\"ÔºåÁ¨¨‰∏Ä‰∏™Áª¥Â∫¶‰∏∫ËæìÂÖ•Ê†∑Êú¨ÁöÑtokenÂ∫èÂàóÔºåÁ¨¨‰∫å‰∏™Áª¥Â∫¶‰∏∫ÊØè‰∏™Â∫èÂàó‰∏≠ÊØè‰∏™tokenÁöÑstartÂíåendÁöÑ‰ΩçÁΩÆ„ÄÇ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "raw_datasets[\"train\"][2:6][\"answers\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(inputs.sequence_ids(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "sequence_ids ÁöÑ‰ΩúÁî®\n",
        "sequence_idsÔºöËøîÂõû‰∏Ä‰∏™ÂàóË°®ÔºåÂÖ∂‰∏≠ÁöÑÊØè‰∏™ÂÖÉÁ¥†ÂØπÂ∫î‰∏Ä‰∏™tokenÂú®Êï¥‰∏™ËæìÂÖ•Â∫èÂàó‰∏≠ÁöÑ‰ΩçÁΩÆÊ†áËØÜ„ÄÇÈÄöÂ∏∏ÔºåÂàóË°®ÁöÑÊØè‰∏™ÂÖÉÁ¥†ÂÄº‰∏∫Ôºö\n",
        "0 Ë°®Á§∫ËØ•tokenÂ±û‰∫éÁ¨¨‰∏Ä‰∏™ËæìÂÖ•Â∫èÂàóÔºà‰æãÂ¶Ç questionÔºâ„ÄÇ\n",
        "1 Ë°®Á§∫ËØ•tokenÂ±û‰∫éÁ¨¨‰∫å‰∏™ËæìÂÖ•Â∫èÂàóÔºà‰æãÂ¶Ç contextÔºâ„ÄÇ\n",
        "None Ë°®Á§∫ËØ•‰ΩçÁΩÆÊ≤°ÊúâÂØπÂ∫îÁöÑtokenÔºà‰æãÂ¶ÇÂ°´ÂÖÖÁöÑÈÉ®ÂàÜÊàñÂàÜÈöîÁ¨¶Ôºâ„ÄÇ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JdpUHtk0drnR",
        "outputId": "d70cc1e9-d037-44b0-f80e-b9b1e9dd2565"
      },
      "outputs": [],
      "source": [
        "answers = raw_datasets[\"train\"][2:6][\"answers\"]\n",
        "start_positions = []\n",
        "end_positions = []\n",
        "\n",
        "for i, offset in enumerate(inputs[\"offset_mapping\"]): #i‰∏∫Ê†∑Êú¨ÁöÑÂ∫èÂàóÁöÑÁ¥¢Âºï, Ëøô‰∏™Ê†∑Êú¨ÊòØÂ∑≤ÁªèÁªèËøáÊà™Êñ≠ÂíåoverflowÂ§ÑÁêÜÁöÑ‰ªéÂéüÂßãÁöÑÈóÆÈ¢ò‰∏ä‰∏ãÊñá‰∏≠‰∫ßÁîüÁöÑÊñ∞ÁöÑÊ†∑Êú¨Â∫èÂàó\n",
        "    sample_idx = inputs[\"overflow_to_sample_mapping\"][i]#Á¨¨i‰∏™Ê†∑Êú¨ÂØπÂ∫îÁöÑÂéüÂßãÁöÑquestionÔºåcontextÁöÑÁ¥¢Âºï‰ΩçÁΩÆ\n",
        "    answer = answers[sample_idx]#ÈíàÂØπËøô‰∏™questionÔºåcontextÊâÄÂØπÂ∫îÊ†∑Êú¨ÁöÑÈóÆÈ¢òÁöÑÁ≠îÊ°à\n",
        "    start_char = answer[\"answer_start\"][0] #answer_start‰ª£Ë°®ÈóÆÈ¢òÂú®context‰∏≠ÁöÑoffset\n",
        "    end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])#ÈóÆÁ≠îÁöÑÁªìÊùüÁöÑoffset\n",
        "    sequence_ids = inputs.sequence_ids(i) #ÂΩìÂâçÊ†∑Êú¨Â∫èÂàóÁöÑÊØè‰∏Ätoken‰ΩçÁΩÆÊòØÊù•Ëá™questionÔºà0ÔºâËøòÊòØcontextÔºà1Ôºâ\n",
        "\n",
        "    # Find the start and end of the context\n",
        "    idx = 0\n",
        "    while sequence_ids[idx] != 1:\n",
        "        idx += 1\n",
        "    context_start = idx\n",
        "    while sequence_ids[idx] == 1:\n",
        "        idx += 1\n",
        "    context_end = idx - 1\n",
        "\n",
        "    # If the answer is not fully inside the context, label is (0, 0)\n",
        "    # Â¶ÇÊûú‰∏ä‰∏ãÊñáÂºÄÂßã‰ΩçÁΩÆÁöÑtokenÂ∫èÂàóÁöÑÁ¨¨‰∏Ä‰∏™Â≠óÁ¨¶ÁöÑ‰ΩçÁΩÆÊØîstart_charË¶ÅÂ§ßÔºå‰πüÂ∞±ÊòØÁ≠îÊ°àÂºÄÂßãÁöÑ‰ΩçÁΩÆÂú®context‰πãÂâç\n",
        "    # ÊàñËÄÖ‰∏ä‰∏ãÊñáÁªìÊùü‰ΩçÁΩÆÁöÑtokenÂ∫èÂàóÁöÑÊúÄÂêé‰∏Ä‰∏™Â≠óÁ¨¶ÁöÑ‰ΩçÁΩÆÊØîend_charË¶ÅÂ∞èÔºå‰πüÂ∞±ÊòØÁ≠îÊ°àÁªìÊùüÁöÑÂ≠óÁ¨¶‰ΩçÁΩÆÂú®context‰πãÂêé\n",
        "    if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
        "        start_positions.append(0)\n",
        "        end_positions.append(0)\n",
        "    else:\n",
        "        # Otherwise it's the start and end token positions \n",
        "        #ÊâæÂá∫Á≠îÊ°àÁöÑËµ∑ÂßãÂíåÁªìÊùü‰ΩçÁΩÆÔºå‰∏çÊòØ‰ª•offsetÁöÑÂ≠óÁ¨¶‰∏∫Âçï‰ΩçÔºåËÄåÊòØ‰ª•tokenÁöÑÁ¥¢Âºï‰∏∫Âçï‰Ωç\n",
        "        idx = context_start\n",
        "        while idx <= context_end and offset[idx][0] <= start_char:\n",
        "            idx += 1\n",
        "        start_positions.append(idx - 1)\n",
        "\n",
        "        idx = context_end\n",
        "        while idx >= context_start and offset[idx][1] >= end_char:\n",
        "            idx -= 1\n",
        "        end_positions.append(idx + 1)\n",
        "\n",
        "start_positions, end_positions  #ÈÄöËøáËøô‰∏™ÂáΩÊï∞ÊâæÂá∫‰∫ÜÊØè‰∏™ÁîüÊàêÂ∫èÂàóÁ≠îÊ°àÂú®ÂêÑËá™ÁöÑÂ∫èÂàó‰∏≠ÁöÑËµ∑Âßã‰ΩçÁΩÆÔºåÂ¶ÇÊûúÁ≠îÊ°à‰∏çÂÆåÂÖ®Âú®contextÈáåÔºåÂàôÊ†áËÆ∞‰∏∫Ôºà0Ôºå0ÔºâÔºå‰ΩçÁΩÆÁ¥¢ÂºïÊòØÁõ∏ÂØπ‰∫éÊï¥‰∏™Â∫èÂàóËÄåË®ÄÁöÑ„ÄÇ19‰∏™featureÂ∞±Êúâ19‰∏™Á≠îÊ°à"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDmXS0QHdrnS",
        "outputId": "8d2e084f-125d-48f3-d8d8-dc06d67d87b4"
      },
      "outputs": [],
      "source": [
        "idx = 0\n",
        "sample_idx = inputs[\"overflow_to_sample_mapping\"][idx]\n",
        "answer = answers[sample_idx][\"text\"][0]\n",
        "\n",
        "start = start_positions[idx]\n",
        "end = end_positions[idx]\n",
        "labeled_answer = tokenizer.decode(inputs[\"input_ids\"][idx][start : end + 1])\n",
        "\n",
        "print(f\"Theoretical answer: {answer}, labels give: {labeled_answer}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wj6DE7-bdrnS",
        "outputId": "e87a210b-cb4a-4e1c-bd1f-a2951b073f26"
      },
      "outputs": [],
      "source": [
        "idx = 5\n",
        "sample_idx = inputs[\"overflow_to_sample_mapping\"][idx]\n",
        "answer = answers[sample_idx][\"text\"][0]\n",
        "\n",
        "decoded_example = tokenizer.decode(inputs[\"input_ids\"][idx])\n",
        "print(f\"Theoretical answer: {answer}, decoded example: {decoded_example}\")\n",
        "\n",
        "start = start_positions[idx]\n",
        "end = end_positions[idx]\n",
        "labeled_answer = tokenizer.decode(inputs[\"input_ids\"][idx][start : end + 1])\n",
        "\n",
        "print(f\"Theoretical answer: {answer}, labels give: {labeled_answer}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ndFD-M0drnS"
      },
      "outputs": [],
      "source": [
        "max_length = 384\n",
        "stride = 128\n",
        "\n",
        "\n",
        "def preprocess_training_examples(examples):\n",
        "    questions = [q.strip() for q in examples[\"question\"]]\n",
        "    inputs = tokenizer(\n",
        "        questions,\n",
        "        examples[\"context\"],\n",
        "        max_length=max_length,\n",
        "        truncation=\"only_second\",\n",
        "        stride=stride,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    offset_mapping = inputs.pop(\"offset_mapping\") #‰∫åÁª¥ÂàóË°®Ôºå\n",
        "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
        "    answers = examples[\"answers\"]\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "    for i, offset in enumerate(offset_mapping):#‰∫åÁª¥ÂàóË°®ÔºåiË°®Á§∫Á¨¨Âá†‰∏™Ê†∑Êú¨\n",
        "        sample_idx = sample_map[i]#Ë°®Á§∫ÂΩìÂâçÂ∫èÂàóÊâÄÂØπÂ∫îÁöÑÂéüÂßãÁöÑÈóÆÁ≠îÁöÑÊ†∑Êú¨\n",
        "        answer = answers[sample_idx]#ÊâæÂá∫ÂéüÂßãÈóÆÁ≠îÁ≠îÊ°à\n",
        "        start_char = answer[\"answer_start\"][0]\n",
        "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
        "        sequence_ids = inputs.sequence_ids(i)\n",
        "\n",
        "        # Find the start and end of the context\n",
        "        idx = 0\n",
        "        while sequence_ids[idx] != 1:\n",
        "            idx += 1\n",
        "        context_start = idx\n",
        "        while sequence_ids[idx] == 1:\n",
        "            idx += 1\n",
        "        context_end = idx - 1\n",
        "\n",
        "        # If the answer is not fully inside the context, label is (0, 0)\n",
        "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
        "            start_positions.append(0)\n",
        "            end_positions.append(0)\n",
        "        else:\n",
        "            # Otherwise it's the start and end token positions\n",
        "            idx = context_start\n",
        "            while idx <= context_end and offset[idx][0] <= start_char:\n",
        "                idx += 1\n",
        "            start_positions.append(idx - 1)\n",
        "\n",
        "            idx = context_end\n",
        "            while idx >= context_start and offset[idx][1] >= end_char:\n",
        "                idx -= 1\n",
        "            end_positions.append(idx + 1)\n",
        "\n",
        "    inputs[\"start_positions\"] = start_positions\n",
        "    inputs[\"end_positions\"] = end_positions\n",
        "    return inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "raw_datasets[\"train\"].column_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lx-p4c71drnS",
        "outputId": "6bb450de-caba-47dc-dd61-637c72a63f4b"
      },
      "outputs": [],
      "source": [
        "train_dataset = raw_datasets[\"train\"].map(\n",
        "    preprocess_training_examples,\n",
        "    batched=True,\n",
        "    remove_columns=raw_datasets[\"train\"].column_names,\n",
        ")\n",
        "len(raw_datasets[\"train\"]), len(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69vijAC2drnS"
      },
      "outputs": [],
      "source": [
        "def preprocess_validation_examples(examples):\n",
        "    questions = [q.strip() for q in examples[\"question\"]]\n",
        "    inputs = tokenizer(\n",
        "        questions,\n",
        "        examples[\"context\"],\n",
        "        max_length=max_length,\n",
        "        truncation=\"only_second\",\n",
        "        stride=stride,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
        "    example_ids = []\n",
        "\n",
        "    for i in range(len(inputs[\"input_ids\"])):\n",
        "        sample_idx = sample_map[i]\n",
        "        example_ids.append(examples[\"id\"][sample_idx])\n",
        "\n",
        "        sequence_ids = inputs.sequence_ids(i)\n",
        "        offset = inputs[\"offset_mapping\"][i]\n",
        "        inputs[\"offset_mapping\"][i] = [\n",
        "            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset) #Â¶ÇÊûúÊòØcontextÔºåÂàô‰∏∫ÔºàstartÔºåendÔºâÔºåÂ¶Ç‰∏∫questionÊú¨Ë∫´Âàô‰∏∫None\n",
        "        ]\n",
        "\n",
        "    inputs[\"example_id\"] = example_ids\n",
        "    return inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJlQmTqSdrnT",
        "outputId": "6dded351-ad8c-4a6b-de59-4f78b93608b0"
      },
      "outputs": [],
      "source": [
        "validation_dataset = raw_datasets[\"validation\"].map(\n",
        "    preprocess_validation_examples,\n",
        "    batched=True,\n",
        "    remove_columns=raw_datasets[\"validation\"].column_names,\n",
        ")\n",
        "len(raw_datasets[\"validation\"]), len(validation_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKLAP97mdrnT"
      },
      "outputs": [],
      "source": [
        "small_eval_set = raw_datasets[\"validation\"].select(range(100))\n",
        "trained_checkpoint = \"distilbert-base-cased-distilled-squad\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(trained_checkpoint)\n",
        "eval_set = small_eval_set.map(\n",
        "    preprocess_validation_examples,\n",
        "    batched=True,\n",
        "    remove_columns=raw_datasets[\"validation\"].column_names,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPHB4hVEdrnT"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "AutoModelForQuestionAnswering ÂèäÂÖ∂Áõ∏ÂÖ≥ÁöÑÊ®°ÂûãÂú®ËøõË°åÊé®ÁêÜÊó∂ÔºåÊîØÊåÅÂ≠óÂÖ∏ÂΩ¢ÂºèÁöÑËæìÂÖ•Êï∞ÊçÆ„ÄÇËøôÁßçÂ≠óÂÖ∏ÈÄöÂ∏∏ÂåÖÂê´Ê®°ÂûãÊâÄÈúÄÁöÑ‰∏çÂêåËæìÂÖ•Âº†ÈáèÔºåÂ¶Ç input_ids„ÄÅattention_mask Á≠â„ÄÇ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Klq0TggJdrnT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForQuestionAnswering\n",
        "\n",
        "eval_set_for_model = eval_set.remove_columns([\"example_id\", \"offset_mapping\"])\n",
        "eval_set_for_model.set_format(\"torch\")\n",
        "\n",
        "print(eval_set_for_model.column_names)\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "batch = {k: eval_set_for_model[k].to(device) for k in eval_set_for_model.column_names}\n",
        "trained_model = AutoModelForQuestionAnswering.from_pretrained(trained_checkpoint).to(\n",
        "    device\n",
        ")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = trained_model(**batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qpp_uLHVdrnT"
      },
      "outputs": [],
      "source": [
        "start_logits = outputs.start_logits.cpu().numpy()\n",
        "end_logits = outputs.end_logits.cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sj4uP9sXdrnT"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "\n",
        "example_to_features = collections.defaultdict(list)\n",
        "for idx, feature in enumerate(eval_set):\n",
        "    example_to_features[feature[\"example_id\"]].append(idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yeUf-gSPdrnU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "n_best = 20\n",
        "max_answer_length = 30\n",
        "predicted_answers = []\n",
        "\n",
        "for example in small_eval_set:\n",
        "    example_id = example[\"id\"]\n",
        "    context = example[\"context\"]\n",
        "    answers = []\n",
        "\n",
        "    for feature_index in example_to_features[example_id]:\n",
        "        start_logit = start_logits[feature_index]\n",
        "        end_logit = end_logits[feature_index]\n",
        "        offsets = eval_set[\"offset_mapping\"][feature_index]\n",
        "\n",
        "        start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
        "        end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
        "        for start_index in start_indexes:\n",
        "            for end_index in end_indexes:\n",
        "                # Skip answers that are not fully in the context\n",
        "                if offsets[start_index] is None or offsets[end_index] is None:\n",
        "                    continue\n",
        "                # Skip answers with a length that is either < 0 or > max_answer_length.\n",
        "                if (\n",
        "                    end_index < start_index\n",
        "                    or end_index - start_index + 1 > max_answer_length\n",
        "                ):\n",
        "                    continue\n",
        "\n",
        "                answers.append(\n",
        "                    {\n",
        "                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
        "                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
        "                    }\n",
        "                )\n",
        "\n",
        "    best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
        "    predicted_answers.append({\"id\": example_id, \"prediction_text\": best_answer[\"text\"]})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXZu8MczdrnU"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"squad\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CsTmr3wcdrnU"
      },
      "outputs": [],
      "source": [
        "theoretical_answers = [\n",
        "    {\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in small_eval_set\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DavjN4PdrnU",
        "outputId": "5a2b12f0-11d1-4a54-f8dd-4af162dc8451"
      },
      "outputs": [],
      "source": [
        "print(predicted_answers[0])\n",
        "print(theoretical_answers[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H74MlmUUdrnU",
        "outputId": "fb8d2de4-a249-4795-ad98-8abc83a87bb1"
      },
      "outputs": [],
      "source": [
        "metric.compute(predictions=predicted_answers, references=theoretical_answers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "El7C8bGhdrnV"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "\n",
        "def compute_metrics(start_logits, end_logits, features, examples):\n",
        "    example_to_features = collections.defaultdict(list)\n",
        "    for idx, feature in enumerate(features):\n",
        "        example_to_features[feature[\"example_id\"]].append(idx)\n",
        "\n",
        "    predicted_answers = []\n",
        "    for example in tqdm(examples):\n",
        "        example_id = example[\"id\"]\n",
        "        context = example[\"context\"]\n",
        "        answers = []\n",
        "\n",
        "        # Loop through all features associated with that example\n",
        "        for feature_index in example_to_features[example_id]:\n",
        "            start_logit = start_logits[feature_index]\n",
        "            end_logit = end_logits[feature_index]\n",
        "            offsets = features[feature_index][\"offset_mapping\"]\n",
        "\n",
        "            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
        "            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
        "            for start_index in start_indexes:\n",
        "                for end_index in end_indexes:\n",
        "                    # Skip answers that are not fully in the context\n",
        "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
        "                        continue\n",
        "                    # Skip answers with a length that is either < 0 or > max_answer_length\n",
        "                    if (\n",
        "                        end_index < start_index\n",
        "                        or end_index - start_index + 1 > max_answer_length\n",
        "                    ):\n",
        "                        continue\n",
        "\n",
        "                    answer = {\n",
        "                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
        "                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
        "                    }\n",
        "                    answers.append(answer)\n",
        "\n",
        "        # Select the answer with the best score\n",
        "        if len(answers) > 0:\n",
        "            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
        "            predicted_answers.append(\n",
        "                {\"id\": example_id, \"prediction_text\": best_answer[\"text\"]}\n",
        "            )\n",
        "        else:\n",
        "            predicted_answers.append({\"id\": example_id, \"prediction_text\": \"\"})\n",
        "\n",
        "    theoretical_answers = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in examples]\n",
        "    return metric.compute(predictions=predicted_answers, references=theoretical_answers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-K1-kPJAdrnV",
        "outputId": "9195ae3d-b6ae-4a28-dd48-620ac69e06e8"
      },
      "outputs": [],
      "source": [
        "compute_metrics(start_logits, end_logits, eval_set, small_eval_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RnZ43WIQdrne"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xr8zdfXgdrne"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAQt6MQfdrne"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "args = TrainingArguments(\n",
        "    \"bert-finetuned-squad\",\n",
        "    evaluation_strategy=\"no\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    fp16=False,\n",
        "    push_to_hub=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6bmbvBHdrnf"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=validation_dataset,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZgX1ZTY_drnf",
        "outputId": "b6f45deb-eac2-42ab-cede-67816841f077"
      },
      "outputs": [],
      "source": [
        "predictions, _, _ = trainer.predict(validation_dataset)\n",
        "start_logits, end_logits = predictions\n",
        "compute_metrics(start_logits, end_logits, validation_dataset, raw_datasets[\"validation\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6z8q8Vardrnf",
        "outputId": "15d8a2b4-ff0f-48f8-ee82-bd932e584bf5"
      },
      "outputs": [],
      "source": [
        "trainer.push_to_hub(commit_message=\"Training complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApbgPVCWdrnf"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from transformers import default_data_collator\n",
        "\n",
        "train_dataset.set_format(\"torch\")\n",
        "validation_set = validation_dataset.remove_columns([\"example_id\", \"offset_mapping\"])\n",
        "validation_set.set_format(\"torch\")\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    shuffle=True,\n",
        "    collate_fn=default_data_collator,\n",
        "    batch_size=8,\n",
        ")\n",
        "eval_dataloader = DataLoader(\n",
        "    validation_set, collate_fn=default_data_collator, batch_size=8\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ymfdp15Bdrnf"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWC0tpWrdrnf"
      },
      "outputs": [],
      "source": [
        "from torch.optim import AdamW\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9OsF5rLdrng"
      },
      "outputs": [],
      "source": [
        "from accelerate import Accelerator\n",
        "\n",
        "accelerator = Accelerator(fp16=True)\n",
        "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
        "    model, optimizer, train_dataloader, eval_dataloader\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zdvabmGSdrng"
      },
      "outputs": [],
      "source": [
        "from transformers import get_scheduler\n",
        "\n",
        "num_train_epochs = 3\n",
        "num_update_steps_per_epoch = len(train_dataloader)\n",
        "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
        "\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=num_training_steps,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5odvCZoldrng",
        "outputId": "c5fd1901-a9d5-4ea2-c3d9-9e4e06e1e9fb"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import Repository, get_full_repo_name\n",
        "\n",
        "model_name = \"bert-finetuned-squad-accelerate\"\n",
        "repo_name = get_full_repo_name(model_name)\n",
        "repo_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I93x6C1cdrng"
      },
      "outputs": [],
      "source": [
        "output_dir = \"bert-finetuned-squad-accelerate\"\n",
        "repo = Repository(output_dir, clone_from=repo_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVyTrHbodrng"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "import torch\n",
        "\n",
        "progress_bar = tqdm(range(num_training_steps))\n",
        "\n",
        "for epoch in range(num_train_epochs):\n",
        "    # Training\n",
        "    model.train()\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        accelerator.backward(loss)\n",
        "\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        progress_bar.update(1)\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    start_logits = []\n",
        "    end_logits = []\n",
        "    accelerator.print(\"Evaluation!\")\n",
        "    for batch in tqdm(eval_dataloader):\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**batch)\n",
        "\n",
        "        start_logits.append(accelerator.gather(outputs.start_logits).cpu().numpy())\n",
        "        end_logits.append(accelerator.gather(outputs.end_logits).cpu().numpy())\n",
        "\n",
        "    start_logits = np.concatenate(start_logits)\n",
        "    end_logits = np.concatenate(end_logits)\n",
        "    start_logits = start_logits[: len(validation_dataset)]\n",
        "    end_logits = end_logits[: len(validation_dataset)]\n",
        "\n",
        "    metrics = compute_metrics(\n",
        "        start_logits, end_logits, validation_dataset, raw_datasets[\"validation\"]\n",
        "    )\n",
        "    print(f\"epoch {epoch}:\", metrics)\n",
        "\n",
        "    # Save and upload\n",
        "    accelerator.wait_for_everyone()\n",
        "    unwrapped_model = accelerator.unwrap_model(model)\n",
        "    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
        "    if accelerator.is_main_process:\n",
        "        tokenizer.save_pretrained(output_dir)\n",
        "        repo.push_to_hub(\n",
        "            commit_message=f\"Training in progress epoch {epoch}\", blocking=False\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzxWN_WXdrng"
      },
      "outputs": [],
      "source": [
        "accelerator.wait_for_everyone()\n",
        "unwrapped_model = accelerator.unwrap_model(model)\n",
        "unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bu3Z6c1hdrnh",
        "outputId": "f27ba3e1-7f2b-4b80-df52-eb908704b231"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Replace this with your own checkpoint\n",
        "model_checkpoint = \"huggingface-course/bert-finetuned-squad\"\n",
        "question_answerer = pipeline(\"question-answering\", model=model_checkpoint)\n",
        "\n",
        "context = \"\"\"\n",
        "ü§ó Transformers is backed by the three most popular deep learning libraries ‚Äî Jax, PyTorch and TensorFlow ‚Äî with a seamless integration\n",
        "between them. It's straightforward to train your models with one before loading them for inference with the other.\n",
        "\"\"\"\n",
        "question = \"Which deep learning libraries back ü§ó Transformers?\"\n",
        "question_answerer(question=question, context=context)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Question answering (PyTorch)",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
