# Transformer Implementation


## References

pytorch实现的MultiHeadAttention:pytorch\torch\nn\modules\activation.py文件中的MultiheadAttention类
https://github.com/facebookresearch/fairseq
https://github.com/joeynmt/joeynmt/blob/main/joeynmt/transformer_layers.py
https://github.com/joeynmt/joeynmt.git 支持训练，测试和翻译的transformer autoencoder的实现
https://colab.research.google.com/drive/1mHUprXsx5VpNiGrrdg0yYLtOP4Y5jRiB?pli=1&authuser=1#scrollTo=zBaKFoTbvC5D
