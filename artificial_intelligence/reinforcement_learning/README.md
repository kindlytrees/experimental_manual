# README

## 内容总结

- 马尔可夫决策过程的定义，马尔可夫奖励过程的价值函数的bellman递归方程迭代，以及在马尔可夫奖励过程的基础之上添加行为a的马尔可夫决策过程中的两种价值函数的迭代（状态价值函数和Q函数（状态行为值函数）
- 基于有模型的马尔可夫决策过程实现策略迭代和价值迭代，求解最优策略，实验中基于定义好的cliffwalking的马尔可夫决策过程的模型，去求解最优策略
- 实际的应用场景，此时的可以基于无模型的马尔可夫模型，基于蒙特卡洛的统计方法或时序差分的方法进行广义的策略迭代求解最优策略，基于表格的方法有sarsa和qlearning，该实验同样基于cliffwalking模拟环境，但没有显示定义马尔可夫决策过程中的状态转移概率矩阵等模型参数，而是采用了在线模拟环境中的数据采集和动态的收集数据，并进行GPI基于Q table实现了最优策略的查找
- 环境中的状态在高维连续空间，用离散的状态不便描述，采用了神经网络来实现Q函数的近似，并通过和QLearning类似的思想基于时序差分的机制实现了Q函数的学习
- 策略梯度算法采用了直接学习最优策略即状态s下不同动作的概率，采用神经网络来近似策略函数，并通过期望回报最大化的思路进行实现，在实现中可以基于蒙特卡洛的基于回合episode的在线数据采集策略进行实现，策略梯度的经典算法为REINFORCE算法
- actor-critic算法框架，基于actor的策略网络，以及critic的值网络，用值网络的时序差分的结果作为策略网络中的概率对数的权重，同时训练策略网路和值网络
- trpo算法，基于期望回报最大化的优化目标和参数更新的可信区域约束，基于拉格朗日乘数子实现参数的迭代，相比与REINFORCE算法有更好的稳定性，但是其基于二次函数的优化计算性能方面有局限
- ppo算法，基于重要性采样机制，采用异策略的机制，两个策略网络，一个用于采集数据，一个用于策略网络的训练，同时也训练价值网络用于策略网络中的对数概率权重定义，其中采集数据的网络生成的数据会多次用于训练网络的参数更新，如10次迭代，这10次迭代的数据复用，10次迭代完成后基于新的参数再迭代采集数据，直至训练结束完成

On-policy vs. Off-policy的核心区别：
- 在于数据来源。On-policy用当前策略产生的数据学习；Off-policy可以用任何策略（通常是旧的、探索性的策略）产生的数据学习。
- 数据利用率：这是两者最大的实践差异。Off-policy因为有Replay Buffer，可以反复利用旧数据，数据利用率极高。On-policy每更新一次策略，就必须丢弃旧数据重新采样，数据利用率低。
PPO确实可以被看作是一种**“几乎在策略” (Near On-Policy)** 或者说**“带有微小离策略修正的在策略” (On-policy with minor off-policy correction)** 算法。
下面，我将对您的这个精准观察进行详细的补充说明。

```
pip install pygame
pip install gymnasium
pip install matplotlib
pip install tqdm
```